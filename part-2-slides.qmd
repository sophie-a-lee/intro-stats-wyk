---
title: "Introduction to Statistics"
subtitle: "Part 2: Exploring trends and going beyond the sample" 
author: Sophie Lee
format: 
  revealjs:
    incremental: true
    slide-number: true
    theme: custom.scss
    width: 1920
    height: 1080
knitr:
  opts_chunk: 
    fig.align: center
    message: false
    dev: png
    dev.args: { bg: "transparent" }
---

```{r}
#| label: setup
#| include: false

pacman::p_load(tidyverse, flextable, kableExtra, ggforce,
               palmerpenguins)

# Theme for output
theme_stats_thinking <- function() {
  theme_minimal() +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 15),
        axis.line = element_line(colour = "grey30"),
        panel.grid.major = element_line(colour = "grey50"),
        panel.grid.minor = element_line(colour = "grey75"),
        panel.background = element_blank(),
        legend.title = element_text(size = 15),
        legend.text = element_text(size = 12))
}

csp_2020 <- read_csv("data/CSP_2020.csv")
```

# {.chapter-theme}

::: r-fit-text
Recap of Part 1
::: 

# Before the analysis
- How to formulate a **research question**
  - **P**opulation
  - **I**ntervention
  - **C**omparison
  - **O**utcome
- Need a **random sample** from the target population
- Outcome must be **measurable** and **relevant**

# Before the analysis
- Consider **biases**
  - Lead to **invalid** results
  - Unable to make inferences and answer the research question
- Be aware of **missing data** 
  - Some missing data can lead to **biases**
  - Even where they don't reduces sample size and **power**
  - Think of potential reasons and be **transparent**
  
# Summary statistics
- Quantify aspects of the **sample**
- Not used to answer research question
- Summaries of categorical variables include **proportion**, **percentage** and **rate**
  - Choice of summary up to you but have different **interpretations**
  
# Summary statistics
- Summaries of numeric variables quantify the **centre** and **spread** of a sample
- Choice of summary depends on the **distribution** of the sample

# 
:::{.fragment}
```{r}
#|label: normal-density


normal_data <- tibble(x = seq(-4, 4, length=100),
                      y = dnorm(x))

normal_curve <- ggplot(normal_data) +
  geom_density(aes(x = x, y = y), stat = "identity",
               fill = "thistle") +
  theme_void() 

normal_curve
```
:::

# Summary statistics
- Measure of centre is given by the **mean** (if normally distributed) or **median**
- Measure of spread is given by the **SD** (if normally distributed) or **IQR**
- Distribution checked using a **histogram** or, if we don't have the data, estimating the **range**

# Normal distribution
```{r}
#|label: normal-with-sd


ggplot(normal_data) +
  geom_density(aes(x = x, y = y), stat = "identity",
               fill = "thistle") +
  geom_vline(xintercept = -3:3, colour = "grey55") +
  scale_x_continuous(name = "", breaks = -4:4,
                     labels = c("", "-3sd", "-2sd", "-1sd", "Mean", 
                                "1sd", "2sd", "3sd", "")) +
  theme_void() +
  theme(axis.text.x = element_text(size = 15))
```

# Normal distribution
```{r}
#| label: normal-sd-95

normal_sd <- mutate(normal_data,
                    y1 = c(rep(0, 37), y[38:63], rep(0, 37)),
                    y2 = c(rep(0, 25), y[26:75], rep(0, 25)))

ggplot(data = normal_sd) + 
  geom_density(aes(x = x, y = y2), linewidth = 1,
               stat = "identity", fill = "#efc3e6") +
  geom_line(aes(x = x, y = y), linewidth = 1,
                stat = "identity") +
  geom_hline(yintercept = 0, linewidth = 1) +
  scale_x_continuous(breaks = -3:3, 
                     labels = c("", "-2sd", "", "Mean", "", 
                                "+2sd", "")) +
  annotate("text", x = 0, y = 0.2, label = "95%", size = 7) +
  theme_void() +
  theme(axis.text.x = element_text(size = 15))
```

# {.chapter-theme}

::: r-fit-text
Quantifying differences and 

trends in a sample.
::: 

## Quantifying differences and trends

Most appropriate choice depends on [intention]{.answer}, [type]{.answer} of outcome, [nature]{.answer} of the relationship 

- Comparison of variable between groups
-	Investigating trends over time
-	Relationship between numeric variables

:::{.notes}
Often, our research question will involve a comparison between groups, investigating trends over time, or investigating a relationship between two numeric variables. There are multiple approaches we can use to compare groups but the correct choice will depend on the outcome of interest and the type of relationship we are interested in. This section will describe the most common comparative statistics, their interpretations, and the reasons we may choose to use one approach over another.
:::

## Comparing categorical outcomes

Compare summary statistics (proportions, percentages, or rates) between groups

[[Absolute difference:]{.answer} Subtract values]{.fragment}

[[Relative difference:]{.answer} Divide values]{.fragment}

:::{.notes}
When comparing a categorical variable between groups, we are often comparing the summary measures that were introduced in the previous section: proportions, percentages, and rates. These summaries are either compared using the absolute difference or the relative difference. 
:::

## Comparing categorical outcomes

```{r}
#| label: table-comp-pengiuns
#| echo: false

penguin_cat_comp <- penguins %>% 
  filter(species %in% c("Adelie", "Gentoo"),
         !is.na(sex)) %>% 
  group_by(species, sex) %>% 
  summarise(n_penguins = n()) %>% 
  ungroup() %>% 
  pivot_wider(names_from = sex,
               values_from = n_penguins) %>% 
  mutate(total_penguins = female + male,
         female_perc = round((female / total_penguins) * 100, 2),
         male_perc = round((male / total_penguins) * 100, 2),
         female_clean = paste0(female, " (", female_perc, "%)"),
         male_clean = paste0(male, " (", male_perc, "%)"))

penguin_cat_comp %>% 
  select(species, total_penguins, female_clean, male_clean) %>% 
  kable(col.names = c("Species", "Total", "Female penguins", "Male penguins"),
        align = "lrrr")

```

[[Absolute difference:]{.answer} `{r} paste0(penguin_cat_comp$female_perc[1], "%")` - `{r} paste0(penguin_cat_comp$female_perc[2], "%")`]{.fragment} [ = `{r} paste0(penguin_cat_comp$female_perc[1] - penguin_cat_comp$female_perc[2], "%")`]{.fragment}

[The absolute difference between Adelie and Gentoo penguins is `{r} penguin_cat_comp$female_perc[1] - penguin_cat_comp$female_perc[2]` [percentage points.]{.answer}]{.fragment}

## Comparing categorical outcomes

```{r}
#| label: table-comp-pengiuns2
#| echo: false

penguin_cat_comp %>% 
  select(species, total_penguins, female_clean, male_clean) %>% 
  kable(col.names = c("Species", "Total", "Female penguins", "Male penguins"),
        align = "lrrr")

```

[[Relative difference:]{.answer}  `{r} paste0(penguin_cat_comp$female_perc[1], "%")` $\div$ `{r} paste0(penguin_cat_comp$female_perc[2], "%")`]{.fragment} [ = `{r} round(penguin_cat_comp$female_perc[1] / penguin_cat_comp$female_perc[2], 2)`]{.fragment}

[There were `{r} round(penguin_cat_comp$female_perc[1] / penguin_cat_comp$female_perc[2], 2)` [times]{.answer} more female penguins in the Adelie group than the Gentoo penguin group]{.fragment}

[[No difference]{.answer} would = 1]{.fragment}

## Comparing categorical outcomes

```{r}
#| label: table-comp-pengiuns3
#| echo: false

penguin_cat_comp %>% 
  select(species, total_penguins, female_clean, male_clean) %>% 
  kable(col.names = c("Species", "Total", "Female penguins", "Male penguins"),
        align = "lrrr")

```

[[Relative difference:]{.answer} `{r} paste0(penguin_cat_comp$female_perc[2], "%")` $\div$ `{r} paste0(penguin_cat_comp$female_perc[1], "%")`]{.fragment} [ = `{r} round(penguin_cat_comp$female_perc[2] / penguin_cat_comp$female_perc[1], 2)`]{.fragment}

[There were `{r} round(penguin_cat_comp$female_perc[2] / penguin_cat_comp$female_perc[1], 2)` [times]{.answer} the percentage of female penguins in the Gentoo group than the Adelie penguin group]{.fragment}

[Less than 1: [a reduction]{.answer}]{.fragment}

## Comparing numeric outcomes

Compare measures of centre/average (mean or median) between groups

[Most appropriate depends on [distribution]{.answer} of sample in each group]{.fragment}

[Requires a histogram [per group]{.answer}]{.fragment}

:::{.notes}
The most appropriate method to compare a numeric variable between groups will once again depend on the distribution of the variable. A comparison can either be made using the difference in means, where both groups have a normally distributed sample, or difference in medians, where the samples are skewed.
:::

## Comparing numeric outcomes

```{r}
#| label: histogram-body-sex
#| echo: false

penguins %>% 
  filter(!is.na(body_mass_g), !is.na(sex)) %>% 
  ggplot() +
  geom_histogram(aes(x = body_mass_g), colour = "black", fill = "grey45") +
  scale_x_continuous(breaks = seq(2500, 5500, by = 1000)) +
  facet_wrap(vars(sex)) +
  labs(x = "Body mass (g)", y = "Count") +
  theme_stats_thinking() +
  theme(strip.text = element_text(size = 15, colour = "#ebe8e3"),
        strip.background = element_rect(fill = "#1f2e32"))
```

## Comparing numeric outcomes

Neither the female nor the male group have a normal distribution [&rarr;  compare [medians]{.answer}.]{.fragment}

[Median body mass of female penguins: [`{r} penguins %>% filter(sex == "female") %>% summarise(median(body_mass_g, na.rm = T))`g]{.answer}]{.fragment}

[Median body mass of male penguins: [`{r} penguins %>% filter(sex == "male") %>% summarise(median(body_mass_g, na.rm = T))`g]{.answer}]{.fragment}

[Average difference in body mass: [`{r} penguins %>% filter(sex == "female") %>% summarise(median(body_mass_g, na.rm = T)) - penguins %>% filter(sex == "male") %>% summarise(median(body_mass_g, na.rm = T))`g]{.answer}]{.fragment}

[Female penguins were [`{r} - (penguins %>% filter(sex == "female") %>% summarise(median(body_mass_g, na.rm = T)) - penguins %>% filter(sex == "male") %>% summarise(median(body_mass_g, na.rm = T)))`g]{.answer} lighter on average compared to male penguins in this sample]{.fragment}

## Comparing variables over time

Visualised using line graph

[Common comparisons: [absolute difference, relative difference, or percentage change]{.answer}]{.fragment}

[Choice depends on intention, interpretation differs]{.fragment}

:::{.notes}
When dealing with temporal data, it is important to quantify differences across time as well as visualising them using a line graph. Comparison across time is typically given as an absolute difference between time points, as a relative difference, or this is commonly converted into a percentage change. 
:::

##

```{r}
violent_crime <- read.csv("Data/violent_crime.csv") 

crime_line <- ggplot(data = violent_crime, 
       aes(x = year, y = violent_crime)) +
  geom_line(linewidth = 1) +
  geom_point() +
  scale_x_continuous(breaks = 2010:2020) +
  labs(x = "Year", y = "Number of violent crimes recorded") +
  theme_stats_thinking() 

crime_line
```

:::{.notes}
Recall the line graph given in an earlier section showing the reduction in the number of violent crimes recorded between 2010 and 2020:
This difference can be quantified by comparing the number of violent crimes recorded in 2010 and 2020.
:::

##

```{r}
#| label: crime-line-circle

crime_line +
  geom_mark_circle(aes(filter = year == 2010), fill = "#9a3416") +
  geom_mark_circle(aes(filter = year == 2020), fill = "#9a3416")

```

:::{.notes}
Recall the line graph given in an earlier section showing the reduction in the number of violent crimes recorded between 2010 and 2020:
This difference can be quantified by comparing the number of violent crimes recorded in 2010 (1,841,000 and 2020 1,239,000.)
:::

## Comparing variables over time

**Absolute difference:** 1,841,000 - 1,239,000 [ = 602,000]{.fragment .answer}

[There were [602,000 less violent crimes]{.answer} reported in 2020 compared to 2010]{.fragment}

[**Relative difference:** 1,841,000 &div; 1,239,000]{.fragment} [ = 1.486]{.fragment .answer}

[There were [1.486 times more violent crimes]{.answer} reported in 2010 compared to 2020]{.fragment}

:::{.notes}
The absolute difference is found by subtracting the number of violent crimes recorded in 2020, 1,239,000, by the number in 2010, 1,841,000. There were 602,000 ;ess violent crimes reported in 2020 compared to 2010.

The relative difference is found by dividing one count by the other. In this example, the relative difference (1,841,000 / 1,239,000) is 1.486. This means that there were 1.486 times more violent crimes reported in 2020 compared to 2010. 
:::

## Comparing variables over time

The percentage change can also be found by converting the relative difference

[**Compare relative difference to no difference:**]{.fragment} [ 1.486 - 1]{.fragment} [ = 0.486]{.fragment .answer}

[**Convert the proportion change to percentage:**]{.fragment} [ 0.486 x 100%]{.fragment} 

[ = 48.6%]{.fragment .answer}

[There were [48.6% more violent crimes]{.answer} reported in 2010 than 2020]{.fragment}

:::{.notes}
The percentage change can be found by comparing the relative difference to the null value of 1 (no relative difference). When comparing  2010 to 2020, the relative difference was 0.486 above 1, giving the proportion increase. This can be multiplied by 100 to give a percentage increase of 48.6%. Therefore, there were 48.6% more violent crimes reported in 2010 compared to 2020.
:::

## Comparing variables over time

Percentage reduction found in similar way

[**Relative difference:** 1,239,000 &div; 1,841,000]{.fragment} [ = 0.673]{.fragment .answer}

[**Compare to no difference:**]{.fragment} [ 1 - 0.673]{.fragment} [ = 0.327 (32.7%)]{.fragment .answer}

[There were [32.7% fewer violent crimes]{.answer} reported in 2020 than 2010]{.fragment}

:::{.notes}
The relative difference can also be found by comparing 2020 to 2010 if we want to give the relative or percentage decrease in violent crime. The relative difference (1,239,000 / 1,841,000) is 0.673, so there were 0.673 times the number of crimes reported in 2020 than 2010. This result is not intuitive, so converting the difference into a percentage decrease can make the value easier to interpret. As with an increase, we first find the difference between the relative difference and the null (1 - 0.673). This gives a proportion decrease of 0.327 or a percentage decrease of 32.7%. Therefore, there were 32.7% fewer violent crimes reported in 2020 compared to 2010.
:::

## Relationship between numeric variables

Scatterplot used to visualise trends

[Strength of relationship quantified using [correlation coefficients]{.answer}]{.fragment}

[Choice of coefficients depends on if trend is linear or not]{.fragment}

- **Linear trend:** Pearson’s correlation coefficient
- **Nonlinear trend:** Spearman’s correlation coefficient

:::{.notes}
Correlation coefficients are summary statistics that describe the strength and direction of a relationship between two numeric variables. There are different types of correlation coefficients that exist, the choice of which depends on the nature of the trend it is measuring: is it linear or nonlinear?

The Pearson’s correlation coefficient measures the association between numeric variables if we assume it is linear. It essentially measures how close points lie to the line of best fit added to a scatterplot. The alternative to Pearson’s correlation is Spearman’s correlation coefficient, this measures the general trend upwards or downwards, whether or not this is linear. As with medians and IQRs, Spearman’s correlation coefficient uses less of the data than Pearson’s so we only use it where necessary.
:::

## Correlation coefficients

Take value between [-1]{.answer} and [1]{.answer}

[Correlation of 0 means **no association**]{.fragment}

[Closer coefficient is to +1/-1, the stronger the **positive/negative association** is]{.fragment}

:::{.notes}
Correlation coefficients take a value between -1 and 1. A value of 0 represents no association, values of +/- 1 represent perfect association (a straight or curved line depending on the choice of statistic). Generally, a correlation coefficient will lie between 0 and +/- 1 where the further the value gets from 0, the stronger the relationship is.
:::

##

```{r}
#| label: correlation-1

correlation_data <- tibble(xcor1 = runif(100, 0, 15),
                           ycor1 = xcor1 + 5,
                           ycor_pos = xcor1 + rnorm(100),
                           ycor_1 = -xcor1 + 25,
                           ycor_neg = -xcor1 + rnorm(100) + 25,
                           y_nocor = runif(100, 0, 25),
                           yspearman = (xcor1 + rnorm(100)) ^ 2)

ggplot(data = correlation_data) +
  geom_point(aes(x = xcor1, y = ycor1)) +
  labs(x = "", y = "") + 
  theme_stats_thinking()

```

:::{.notes}
A correlation coefficient is said to show a positive association if the value is above 0. This means as one variable increases, the other also tends to increase:
:::

## 

```{r}
#| label: correlation-pos

ggplot(data = correlation_data) +
  geom_point(aes(x = xcor1, y = ycor_pos)) +
  labs(x = "", y = "") + 
  theme_stats_thinking()
```

## 

```{r}
#| label: correlation-neg1

ggplot(data = correlation_data) +
  geom_point(aes(x = xcor1, y = ycor_1)) +
  labs(x = "", y = "") + 
  theme_stats_thinking()
```

## 

```{r}
#| label: correlation-neg

ggplot(data = correlation_data) +
  geom_point(aes(x = xcor1, y = ycor_neg)) +
  labs(x = "", y = "") + 
  theme_stats_thinking()
```

## 

```{r}
#| label: correlation-nocorr

ggplot(data = correlation_data) +
  geom_point(aes(x = xcor1, y = y_nocor)) +
  labs(x = "", y = "") + 
  theme_stats_thinking()
```

## 

```{r}
#| label: correlation-spearman

ggplot(data = correlation_data) +
  geom_point(aes(x = xcor1, y = yspearman)) +
  labs(x = "", y = "") + 
  theme_stats_thinking()
```

#  {.chapter-theme}

::: r-fit-text
Beyond the sample
:::

## What are inferential statistics?

![](img/stats_inference2.png){height="20cm" width="50cm" fig-align="center"}

:::{.notes}
At the beginning of the course, we saw that one of the main aims of statistics is to make inferences about a target population of interest based on results of analysis applied to a random sample. These inferences require inferential statistics,
:::

## What are inferential statistics?

Inferential statistics make inferences about target population based on a [random, representative]{.answer} sample.

[Combine sample estimates with [sample size]{.answer} and level of [precision]{.answer}]{.fragment}

[Most common inferential statistics:]{.fragment} [[p-values]{.answer} and [confidence intervals]{.answer}]{.fragment}

:::{.notes}
These are estimated by combining results from the random, representative sample taken from the target population, and information about the sample size and precision of the sample estimate.
:::

## Measures of precision

Precision of an estimate quantified by [standard error (SE)]{.answer}

[Based on sample size and sample variability]{.fragment}

[Different formula for each type of estimate (e.g. mean, percentage, difference between means)]{.fragment}

:::{.fragment .bigger .absolute bottom=300 left=750}
$SE(\bar{x}) = \frac{SD}{\sqrt{n}}$
:::

:::{.notes}
Inferential statistics require a measure of how precise a sample estimate is. Precision is quanti􀏐ied using the standard error (SE), calculated using the sample size and sample variability. The formula used to calculate a standard error depends on the type of parameter we wish to obtain from the target.
For example, the standard error of a single mean (𝑆𝐸(𝑥)̄ ) is found by dividing the sample standard deviation (𝑆𝐷) by the square root of the sample size (𝑛):
:::

## Measures of precision

::: columns
:::{.column width="50%"}
Larger SE &rarr; [less precise]{.answer}
:::

:::{.column width="50%"}
[Smaller SE &rarr; [more precise]{.answer}]{.fragment}
:::
:::

:::{.bigger .fragment .absolute left=750}
$SE(\bar{x}) = \frac{SD}{\sqrt{n}}$
:::

:::{.absolute top=500}
[For every parameter of interest:]{.fragment}

- Larger sample, higher precision &rarr; [lower standard error]{.answer}
- More variability, lower precision &rarr; [higher standard error]{.answer}

[Inferential statistics work based on the [central limit theorem]{.answer}]{.fragment}
:::

:::{.notes}
Regardless of the formula used or the parameter of interest, the larger a sample is, the more precise an estimate will be. Conversely, the more varied a sample is, the less precise an estimate is. A precise estimate is represented by a small standard error value. Standard errors are used to estimate inferential statistics (p‑values and con􀏐idence intervals) based on the central limit theorem.
:::

## Central limit theorem

```{r}
#| label: normal-curve

normal_data <- tibble(x = seq(-4, 4, length=100),
                      y = dnorm(x))

normal_curve <- ggplot(normal_data) +
  geom_density(aes(x = x, y = y), stat = "identity",
               fill = "thistle") +
  theme_void() 

normal_curve
```

## Central limit theorem

```{r}
#| label: random-samples

df_arrow <- tibble(y = rep(c(.1, 0), 8),
                   x = rep(runif(8, min = -4, max = 4), each = 2))

normal_curve +
  geom_segment(aes(x = df_arrow$x[1], y = df_arrow$y[1], 
                   xend = df_arrow$x[2], yend = df_arrow$y[2]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2)
```

## Central limit theorem

```{r}
# Create random draws for arrows
normal_curve +
  geom_segment(aes(x = df_arrow$x[1], y = df_arrow$y[1], 
                   xend = df_arrow$x[2], yend = df_arrow$y[2]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[3], y = df_arrow$y[3], 
                   xend = df_arrow$x[4], yend = df_arrow$y[4]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2)
```

## Central limit theorem

```{r}
# Create random draws for arrows
normal_curve +
  geom_segment(aes(x = df_arrow$x[1], y = df_arrow$y[1], 
                   xend = df_arrow$x[2], yend = df_arrow$y[2]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[3], y = df_arrow$y[3], 
                   xend = df_arrow$x[4], yend = df_arrow$y[4]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[5], y = df_arrow$y[5], 
                   xend = df_arrow$x[6], yend = df_arrow$y[6]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2)
```

## Central limit theorem

```{r}
# Create random draws for arrows
normal_curve +
  geom_segment(aes(x = df_arrow$x[1], y = df_arrow$y[1], 
                   xend = df_arrow$x[2], yend = df_arrow$y[2]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[3], y = df_arrow$y[3], 
                   xend = df_arrow$x[4], yend = df_arrow$y[4]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[5], y = df_arrow$y[5], 
                   xend = df_arrow$x[6], yend = df_arrow$y[6]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[7], y = df_arrow$y[7], 
                   xend = df_arrow$x[8], yend = df_arrow$y[8]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2)
```

## Central limit theorem

```{r}
# Create random draws for arrows
normal_curve +
  geom_segment(aes(x = df_arrow$x[1], y = df_arrow$y[1], 
                   xend = df_arrow$x[2], yend = df_arrow$y[2]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[3], y = df_arrow$y[3], 
                   xend = df_arrow$x[4], yend = df_arrow$y[4]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[5], y = df_arrow$y[5], 
                   xend = df_arrow$x[6], yend = df_arrow$y[6]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[7], y = df_arrow$y[7], 
                   xend = df_arrow$x[8], yend = df_arrow$y[8]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[9], y = df_arrow$y[9], 
                   xend = df_arrow$x[10], yend = df_arrow$y[10]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2)
```

## Central limit theorem

```{r}
# Create random draws for arrows
normal_curve +
  geom_segment(aes(x = df_arrow$x[1], y = df_arrow$y[1], 
                   xend = df_arrow$x[2], yend = df_arrow$y[2]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[3], y = df_arrow$y[3], 
                   xend = df_arrow$x[4], yend = df_arrow$y[4]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[5], y = df_arrow$y[5], 
                   xend = df_arrow$x[6], yend = df_arrow$y[6]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[7], y = df_arrow$y[7], 
                   xend = df_arrow$x[8], yend = df_arrow$y[8]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[9], y = df_arrow$y[9], 
                   xend = df_arrow$x[10], yend = df_arrow$y[10]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[11], y = df_arrow$y[11], 
                   xend = df_arrow$x[12], yend = df_arrow$y[12]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2)
```

## Central limit theorem

```{r}
# Create random draws for arrows
normal_curve +
  geom_segment(aes(x = df_arrow$x[1], y = df_arrow$y[1], 
                   xend = df_arrow$x[2], yend = df_arrow$y[2]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[3], y = df_arrow$y[3], 
                   xend = df_arrow$x[4], yend = df_arrow$y[4]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[5], y = df_arrow$y[5], 
                   xend = df_arrow$x[6], yend = df_arrow$y[6]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[7], y = df_arrow$y[7], 
                   xend = df_arrow$x[8], yend = df_arrow$y[8]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[9], y = df_arrow$y[9], 
                   xend = df_arrow$x[10], yend = df_arrow$y[10]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[11], y = df_arrow$y[11], 
                   xend = df_arrow$x[12], yend = df_arrow$y[12]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[13], y = df_arrow$y[13], 
                   xend = df_arrow$x[14], yend = df_arrow$y[14]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2)
```


## Central limit theorem

```{r}
# Create random draws for arrows
normal_curve +
  geom_segment(aes(x = df_arrow$x[1], y = df_arrow$y[1], 
                   xend = df_arrow$x[2], yend = df_arrow$y[2]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[3], y = df_arrow$y[3], 
                   xend = df_arrow$x[4], yend = df_arrow$y[4]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[5], y = df_arrow$y[5], 
                   xend = df_arrow$x[6], yend = df_arrow$y[6]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[7], y = df_arrow$y[7], 
                   xend = df_arrow$x[8], yend = df_arrow$y[8]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[9], y = df_arrow$y[9], 
                   xend = df_arrow$x[10], yend = df_arrow$y[10]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[11], y = df_arrow$y[11], 
                   xend = df_arrow$x[12], yend = df_arrow$y[12]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[13], y = df_arrow$y[13], 
                   xend = df_arrow$x[14], yend = df_arrow$y[14]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2) +
    geom_segment(aes(x = df_arrow$x[15], y = df_arrow$y[15], 
                   xend = df_arrow$x[16], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2)
```

## Central limit theorem

```{r}
# Create random draws for arrows
normal_curve +
  geom_segment(aes(x = mean(df_arrow$x), y = df_arrow$y[15], 
                   xend = mean(df_arrow$x), yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2)
```

## Central limit theorem

```{r}
rand_means <- c(mean(df_arrow$x), mean(runif(8, min = -4, max = 4)),
                mean(runif(8, min = -4, max = 4)),
                mean(runif(8, min = -4, max = 4)),
                mean(runif(8, min = -4, max = 4)))

# Create random draws for arrows
normal_curve +
  geom_segment(aes(x = rand_means[1], y = df_arrow$y[15], 
                   xend = rand_means[1], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2) +
  geom_segment(aes(x = rand_means[2], y = df_arrow$y[15], 
                   xend = rand_means[2], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2)
```

## Central limit theorem
```{r}
# Create random draws for arrows
normal_curve +
  geom_segment(aes(x = rand_means[1], y = df_arrow$y[15], 
                   xend = rand_means[1], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2) +
  geom_segment(aes(x = rand_means[2], y = df_arrow$y[15], 
                   xend = rand_means[2], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2)
```

## Central limit theorem
```{r}
# Create random draws for arrows
normal_curve +
  geom_segment(aes(x = rand_means[1], y = df_arrow$y[15], 
                   xend = rand_means[1], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2) +
  geom_segment(aes(x = rand_means[2], y = df_arrow$y[15], 
                   xend = rand_means[2], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2) + 
  geom_segment(aes(x = rand_means[3], y = df_arrow$y[15], 
                   xend = rand_means[3], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2)
```

## Central limit theorem
```{r}
# Create random draws for arrows
normal_curve +
  geom_segment(aes(x = rand_means[1], y = df_arrow$y[15], 
                   xend = rand_means[1], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2) +
  geom_segment(aes(x = rand_means[2], y = df_arrow$y[15], 
                   xend = rand_means[2], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2) + 
  geom_segment(aes(x = rand_means[3], y = df_arrow$y[15], 
                   xend = rand_means[3], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2) +
  geom_segment(aes(x = rand_means[4], y = df_arrow$y[15], 
                   xend = rand_means[4], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2)
```

## Central limit theorem
```{r}
# Create random draws for arrows
normal_curve +
  geom_segment(aes(x = rand_means[1], y = df_arrow$y[15], 
                   xend = rand_means[1], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2) +
  geom_segment(aes(x = rand_means[2], y = df_arrow$y[15], 
                   xend = rand_means[2], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2) + 
  geom_segment(aes(x = rand_means[3], y = df_arrow$y[15], 
                   xend = rand_means[3], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2) +
  geom_segment(aes(x = rand_means[4], y = df_arrow$y[15], 
                   xend = rand_means[4], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2) +
  geom_segment(aes(x = rand_means[5], y = df_arrow$y[15], 
                   xend = rand_means[5], yend = df_arrow$y[16]),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "black", lwd = 2)
```

## Central limit theorem

```{r}
#| label: sd-to-se

normal_data %>% 
  mutate(se = dnorm(x, sd = y/sqrt(16))) %>% 
  pivot_longer(y:se,
               names_to = "measure",
               values_to = "values") %>% 
  mutate(measure = factor(measure, levels = c("y", "se"),
                          labels = c("SD", "SE"))) %>% 
  ggplot() +
  geom_density(aes(x = x, y = values), stat = "identity",
               fill = "thistle") +
  facet_grid(rows = vars(measure)) +
  theme_stats_thinking() +
  theme(strip.text = element_text(size = 12, colour = "#ebe8e3"),
        strip.background = element_rect(fill = "#1f2e32"))

```

## Confidence intervals

- A range of values the true population parameter is compatible with
- Based on sample estimate, precision, and confidence level

:::{.notes}
A con􀏐idence interval is a range of values that the true population statistic is compatible with based on the sample estimate, precision, and some pre‑de􀏐ined level of con􀏐idence.
The con􀏐idence level can be adjusted depending on how con􀏐ident we wish to be about the true population parameter. 
:::

##

```{r}
#| label: confidence-levels

tibble(level = c("80%", "90%", "95%", "99%", "99.9%"),
       se = c(1.282, 1.645, 1.960, 2.576, 3.291)) %>% 
  kable(col.names = c("Confidence levels", "Number of SEs"), 
        align = "r")
```

## Confidence intervals {.nonincremental}

- A range of values the true population parameter is compatible with
- Based on sample estimate, precision, and confidence level

:::{.fragment}
- Based on central limit theorem, can capture ranges we would expect a percentage of parameter estimates to lie:
:::

:::{.fragment .bigger .absolute left=750}
$\bar{x} \pm 1.96 \times SE(\bar{x})$
:::

:::{.notes}
The con􀏐idence interval is created assuming the central limit theorem. As the hypothetical repeated estimates are assumed to follow a normal distribution, we can use the sample estimate of the parameter and the standard error to obtain ranges within which we would expect a certain percentage of parameter estimates to lie.
:::

##

```{r}
#| label: conf-int-95

normal_sd <- mutate(normal_data,
                    y1 = c(rep(0, 37), y[38:63], rep(0, 37)),
                    y2 = c(rep(0, 25), y[26:75], rep(0, 25)))

conf_int95 <- ggplot(data = normal_sd) + 
  geom_density(aes(x = x, y = y2), linewidth = 1,
               stat = "identity", fill = "#efc3e6") +
  geom_line(aes(x = x, y = y), linewidth = 1,
                stat = "identity") +
  geom_hline(yintercept = 0, linewidth = 1) +
  scale_x_continuous(breaks = -3:3, 
                     labels = c("-3se", "-2se", "-1se", "Sample \nestimate",
                                "+1se", "+2se", "+3se")) +
  annotate("text", x = 0, y = 0.2, label = "95%", size = 7) +
  theme_void() +
  theme(axis.text.x = element_text(size = 15))

conf_int95
```

## Confidence interval example
Let's compare the body mass of our penguins between sexes.

:::{.fragment}
First, we want to check the distribution of these samples:

```{r}
#| label: penguin-sex-comp
#| out-width: "1000px"
#| out-height: "600px"

penguins_sex_comp <- penguins %>% 
  filter(species != "Gentoo",
         !is.na(sex)) 

penguin_sex_hist <- ggplot(data = penguins_sex_comp) +
  geom_histogram(aes(x = body_mass_g, fill = sex), 
                 colour = "black", alpha = .75, bins = 20) +
  scale_fill_brewer(name = "Sex", palette = "Dark2") +
  labs(x = "Body mass (g)", y = "Count") +
  theme_stats_thinking()

penguin_sex_hist
```
:::

## Confidence interval example
```{r}
#| label: penguin-diff-values

penguins_diff <- penguins_sex_comp %>% 
  group_by(sex) %>% 
  summarise(mean_mass = mean(body_mass_g),
            sd_mass = sd(body_mass_g),
            n_pengs = n()) %>% 
  ungroup() %>% 
  mutate(se_cont = (sd_mass / n_pengs)^2)

mean_sex_diff <- penguins_diff[penguins_diff$sex == "male", "mean_mass"] - 
                  penguins_diff[penguins_diff$sex == "female", "mean_mass"]

se_sex_diff <- sqrt(penguins_diff[penguins_diff$sex == "male", "se_cont"] + 
                     penguins_diff[penguins_diff$sex == "female", "se_cont"])
```


Both groups appear to be normally distributed, so we can compare the [means]{.answer}.

[Mean body mass of male penguins: [`{r} round(penguins_diff[penguins_diff$sex == "male", ]$mean_mass, 2)`g]{.answer .fragment}]{.fragment}

[Mean body mass of female penguins: [`{r} round(penguins_diff[penguins_diff$sex == "female", ]$mean_mass, 2)`g]{.answer .fragment}]{.fragment}

[Difference in the means (of the sample): `{r} round(penguins_diff[penguins_diff$sex == "male", ]$mean_mass, 2)`g - `{r} round(penguins_diff[penguins_diff$sex == "female", ]$mean_mass, 2)`g]{.fragment}

[= `{r} round(mean_sex_diff, 2)`g]{.answer .fragment}

## Confidence interval example
Difference in the means (of the sample): `{r} round(mean_sex_diff, 2)`g

[Standard error of the mean difference]{.fragment}[ = `{r} round(se_sex_diff, 2)`g]{.answer .fragment}

[95% confidence interval: `{r} round(mean_sex_diff, 2)` $\pm$ 1.96 $\times$ `{r} round(se_sex_diff, 2)`]{.fragment} 

[ = `{r} paste0(c(round(mean_sex_diff - (1.96 * se_sex_diff), 2), round(mean_sex_diff + (1.96 * se_sex_diff), 2)), "g")`]{.fragment .answer}

[**But what does that mean??**]{.fragment}

## Confidence interval example
95% confidence interval: `{r} paste0(c(round(mean_sex_diff - (1.96 * se_sex_diff), 2), round(mean_sex_diff + (1.96 * se_sex_diff), 2)), "g")`

[We are [95% confident]{.answer} that male penguins were between `{r} round(mean_sex_diff - (1.96 * se_sex_diff), 2)`g and `{r} round(mean_sex_diff + (1.96 * se_sex_diff), 2)`g heavier than female penguins on average.]{.fragment}

[Note that this confidence interval only contains [positive]{.answer} values.]{.answer .fragment}

## p-values

- Probability of obtaining a result as extreme or more extreme as the sample if the null hypothesis is true
- Null hypothesis (H0): no difference/association

:::{.notes}
Another commonly used inferential statistic is the p‑value. A p‑value is the probability of obtaining a sample estimate as extreme, or more extreme, than the current if some null hypothesis (H0) were true.
The null hypothesis is usually ‘no difference’ or ‘no association’, depending on the value being tested.
:::

## p-values

```{r}
#| label: random-se1

normal_curve +
  geom_segment(aes(x = -.3, y = .1, 
                   xend = -.3, yend = 0),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2)
```

:::{.notes}
If we consider the normal distribution of repeated sample estimates, the p‑value is estimated by assuming the null hypothesis is true (and is therefore the peak of the distribution) and measuring how far the sample value is from this relative to the spread of the distribution (the standard error).
The closer the sample estimate is to the null hypothesis, the more likely it was to occur if the null hypothesis were true, and the higher the p‑value
:::

## p-values

```{r}
#| label: random-se2

normal_curve +
  scale_x_continuous(breaks = -3:3, labels = c("-3se", "-2se", "-1se",
                                               "Population \nmean", "+1se",
                                               "+2se", "+3se")) + 
  geom_segment(aes(x = 3.8, y = .1, 
                   xend = 3.8, yend = 0),
               arrow = arrow(length = unit(.3, "cm")), 
               colour = "#9a3416", lwd = 2)
```

:::{.notes}
The further away from the null hypothesis, the less likely it would be to occur and the lower the p‑value.
:::

## p-values 

:::{.nonincremental}
- Probability of obtaining a result as extreme or more extreme as the sample if the null hypothesis is true
- Null hypothesis (H0): no difference/association
:::

:::{.incremental}
- Low p-value: less evidence to support the null hypothesis
  - Very low p-value is known as [statistically significant]{.answer}
:::

## Statistical significance

Often significance is defined by arbitrary cut-off [(usually 0.05)]{.fragment}

[**Be careful with these arbitrary definitions, it is not how probability behaves!**]{.fragment}

[p < 0.05 is significant]{.fragment} [ at the 5% level]{.fragment .answer}

[We never [accept]{.answer} or [reject]{.answer} a null hypothesis]{.fragment}

:::{.notes}
Results are often referred to as statistically signi􀏐icant if a p‑value falls below a certain threshold. 
This threshold is often set to 0.05, or a 5% chance that an estimate as extreme as the one obtained would occur if the null hypothesis were true.
Although arbitrary cut‑offs may be useful in some situations, for example where a decision needs to be taken based on the results, this is not how probability behaves. In reality, there is very little difference between a p‑value of 0.049 (4.9% chance) and 0.051 (a 5.1% chance). Therefore, it is not advised to report ‘accepting’ or ‘rejecting’ a null hypothesis, despite how commonplace this is in some literature.
:::

## p-values example

```{r}
#| label: hist-again
#| out-height: "600px"
#| out-width: "1000px"


penguin_sex_hist
```

## p-values example
As we are comparing groups, our null hypothesis is that there is [no difference]{.answer} in the target population.

[Sample mean difference: [`{r} round(mean_sex_diff, 2)`g]{.answer}]{.fragment}

[Standard error of the difference: [`{r} round(se_sex_diff, 2)`g]{.answer}]{.fragment}

[p-values assume that the null hypothesis is true]{.fragment}

## p-value example

```{r}
#| label: pvalue-normal-h0

tibble(x = seq(-5, 5, by = .1),
       y = dnorm(x)) %>% 
  ggplot() +
  geom_density(aes(x = x, y = y), fill = "thistle", stat = "identity") +
  scale_x_continuous(name = "Difference in mean body mass",
                     breaks = c(-4, -2, 0, 2, 4),
                     labels = c("-4se", "-2se", "0g", "+2se", "+4se")) +
  theme_void() +
  theme(axis.title = element_text(size = 15),
        axis.text = element_text(size = 12))
```

## p-value example
The observed sample mean difference is (`{r} round(mean_sex_diff, 2)` - 0 $\div$ `{r} round(se_sex_diff, 2)`) = [`{r} round(mean_sex_diff / se_sex_diff, 2)` standard errors]{.answer} away from the null hypothesis.

[This is so far that we can't even see it on our histogram!]{.fragment}

[The probability of this happening [if the null were true]{.answer} is VERY VERY small ([p < 0.00000000001]{.answer})]{.fragment}.

[In this case, we would say this difference is [highly significant]{.answer}]{.fragment}

## Relationship between p-values and confidence intervals

Confidence intervals and p-values are based on the same information and so agree with one another

[If a p-value is [above 0.05]{.answer}, the sample estimate is [less than 1.96 SEs away]{.answer}. This means it will be [within the 95% confidence interval]{.answer}]{.fragment}

[If the null hypothesis is [outside the 99% confidence interval]{.answer}, it is [over 2.576 SEs away]{.answer} from the sample estimate so [p < 0.01]{.answer}]{.fragment}

##

```{r}
conf_int95
```

##

```{r}
conf_int95 +
  geom_segment(x = 0.9, xend = 0.9, y = .1, yend = 0,
               arrow = arrow(length = unit(.3, "cm")),
               colour = "#9a3416", lwd = 2)
```

##

```{r}
conf_int95 +
  geom_segment(x = -4.2, xend = -4.2, y = .1, yend = 0,
               arrow = arrow(length = unit(.3, "cm")),
               colour = "#9a3416", lwd = 2)
```

# {.chapter-theme}

::: r-fit-text
Exercise 3:

Inferential statistics
:::

##

![](img/only_connect_table.png){fig-align="center"}

## 

```{r}
#| label: exercise3-counter
#| echo: false


countdown::countdown(minutes = 10,
                     top = 0, font_size = "100px",
                     color_text = "#222222", 
                     color_background = "#fdddb6",
                     color_border = "#9a3416",
                     color_running_text = "#222222", 
                     color_running_background = "#fdddb6",
                     color_running_border = "#9a3416")
```
![](img/only_connect_figs.png){fig-align="center"}